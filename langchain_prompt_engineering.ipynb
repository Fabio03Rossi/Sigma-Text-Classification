{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "import os\n",
    "\n",
    "#!pip install langchain langchain-community langchain-experimental\n",
    "#!pip install llama-cpp-python --force-reinstall --upgrade --no-cache-dir"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-04T15:07:25.648637700Z",
     "start_time": "2025-07-04T14:48:16.772170Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import pipeline\n",
    "from langchain_huggingface import HuggingFaceEndpoint, HuggingFacePipeline\n",
    "from llama_cpp import Llama, LlamaGrammar\n",
    "from langchain_community.llms.llamacpp import LlamaCpp\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"mistralai/Mistral-Small-3.2-24B-Instruct-2506\",\n",
    "    device=\"cuda\",\n",
    "    temperature=0.3,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.2\n",
    ")\n",
    "local_llm = HuggingFacePipeline(\n",
    "    pipeline=pipe\n",
    ")\n",
    "\n",
    "\"\"\"smart_llm = Llama(\n",
    "    model_path=\"models/llama-3.2-3B-Instruct-Q4_0_4_4.gguf\"\n",
    "    #verbose=True\n",
    ")\"\"\""
   ],
   "id": "2ceb6a564e491231",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fbrossi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\fbrossi\\.cache\\huggingface\\hub\\models--mistralai--Mistral-Small-3.2-24B-Instruct-2506. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Fetching 10 files:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-04T15:07:25.648637700Z",
     "start_time": "2025-07-04T12:36:09.424859Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_experimental.smart_llm import SmartLLMChain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "CASSETTE stands for \"Cassette\", \"Cassetto\", \"AC\", \"RC\"\n",
    "then\n",
    "CT stands for \"Cash Transport\", \"Piatto\", \"Piatti\", \"LT\", \"ST\", \"PT\"\n",
    "then\n",
    "NE stands for \"Node Escrow\", \"precassa\", \"nastro\"\n",
    "then\n",
    "NF stands for \"Node Feeder\", \"bocchetta\", \"leva 1\", \"sfogliatore\"\n",
    "then\n",
    "NV stands for \"Node Validator\", \"leva 7\", \"validatore\"\n",
    "then\n",
    "SHUTTER stands for \"Shutter\"\n",
    "\n",
    "Your job is to classify the following input with the labels above defined.\n",
    "You only answer with one single phrase of 50 words and the resulting classification, no more no less.\n",
    "If you're unsure, return UNK.\n",
    "After the answer write \"end\".\n",
    "\n",
    "Input:\n",
    "{input}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"input\"],\n",
    ")\n",
    "\n",
    "chain = prompt | local_llm\n",
    "\n",
    "\"\"\"smart_chain = SmartLLMChain(\n",
    "    llm=smart_llm,\n",
    "    critique_llm=smart_llm,\n",
    "    ideation_llm=smart_llm,\n",
    "    prompt=prompt,\n",
    "    n_ideas=2,\n",
    "    verbose=True\n",
    ")\"\"\""
   ],
   "id": "6e59d4efb9bd235d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'smart_chain = SmartLLMChain(\\n    llm=smart_llm,\\n    critique_llm=smart_llm,\\n    ideation_llm=smart_llm,\\n    prompt=prompt,\\n    n_ideas=2,\\n    verbose=True\\n)'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-04T15:07:25.648637700Z",
     "start_time": "2025-07-04T12:36:09.486321Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def prompt_question(string):\n",
    "    return chain.invoke({\"input\": string})"
   ],
   "id": "ed94a7bbe44e1603",
   "outputs": [],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-04T15:07:25.648637700Z",
     "start_time": "2025-07-04T12:36:09.507759Z"
    }
   },
   "cell_type": "code",
   "source": "prompt_question(\"Riscontrata banconota inceppata nel CT. Rimozione e test ok\")",
   "id": "c6c8ca1ddec78433",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'StringPromptValue' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mTypeError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[56]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[43mprompt_question\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mRiscontrata banconota inceppata nel CT. Rimozione e test ok\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[55]\u001B[39m\u001B[32m, line 2\u001B[39m, in \u001B[36mprompt_question\u001B[39m\u001B[34m(string)\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mprompt_question\u001B[39m(string):\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mchain\u001B[49m\u001B[43m.\u001B[49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\u001B[43m{\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43minput\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstring\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3047\u001B[39m, in \u001B[36mRunnableSequence.invoke\u001B[39m\u001B[34m(self, input, config, **kwargs)\u001B[39m\n\u001B[32m   3045\u001B[39m                 input_ = context.run(step.invoke, input_, config, **kwargs)\n\u001B[32m   3046\u001B[39m             \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m3047\u001B[39m                 input_ = \u001B[43mcontext\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstep\u001B[49m\u001B[43m.\u001B[49m\u001B[43minvoke\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   3048\u001B[39m \u001B[38;5;66;03m# finish the root run\u001B[39;00m\n\u001B[32m   3049\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_core\\runnables\\base.py:4771\u001B[39m, in \u001B[36mRunnableLambda.invoke\u001B[39m\u001B[34m(self, input, config, **kwargs)\u001B[39m\n\u001B[32m   4757\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Invoke this Runnable synchronously.\u001B[39;00m\n\u001B[32m   4758\u001B[39m \n\u001B[32m   4759\u001B[39m \u001B[33;03mArgs:\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m   4768\u001B[39m \u001B[33;03m    TypeError: If the Runnable is a coroutine function.\u001B[39;00m\n\u001B[32m   4769\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   4770\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mfunc\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m-> \u001B[39m\u001B[32m4771\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_with_config\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   4772\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_invoke\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4773\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m   4774\u001B[39m \u001B[43m        \u001B[49m\u001B[43mensure_config\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4775\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4776\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   4777\u001B[39m msg = \u001B[33m\"\u001B[39m\u001B[33mCannot invoke a coroutine function synchronously.Use `ainvoke` instead.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   4778\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(msg)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1940\u001B[39m, in \u001B[36mRunnable._call_with_config\u001B[39m\u001B[34m(self, func, input_, config, run_type, serialized, **kwargs)\u001B[39m\n\u001B[32m   1936\u001B[39m     child_config = patch_config(config, callbacks=run_manager.get_child())\n\u001B[32m   1937\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m set_config_context(child_config) \u001B[38;5;28;01mas\u001B[39;00m context:\n\u001B[32m   1938\u001B[39m         output = cast(\n\u001B[32m   1939\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mOutput\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m-> \u001B[39m\u001B[32m1940\u001B[39m             \u001B[43mcontext\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1941\u001B[39m \u001B[43m                \u001B[49m\u001B[43mcall_func_with_variable_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[arg-type]\u001B[39;49;00m\n\u001B[32m   1942\u001B[39m \u001B[43m                \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1943\u001B[39m \u001B[43m                \u001B[49m\u001B[43minput_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1944\u001B[39m \u001B[43m                \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1945\u001B[39m \u001B[43m                \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1946\u001B[39m \u001B[43m                \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1947\u001B[39m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m,\n\u001B[32m   1948\u001B[39m         )\n\u001B[32m   1949\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m   1950\u001B[39m     run_manager.on_chain_error(e)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_core\\runnables\\config.py:428\u001B[39m, in \u001B[36mcall_func_with_variable_args\u001B[39m\u001B[34m(func, input, config, run_manager, **kwargs)\u001B[39m\n\u001B[32m    426\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m run_manager \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m accepts_run_manager(func):\n\u001B[32m    427\u001B[39m     kwargs[\u001B[33m\"\u001B[39m\u001B[33mrun_manager\u001B[39m\u001B[33m\"\u001B[39m] = run_manager\n\u001B[32m--> \u001B[39m\u001B[32m428\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_core\\runnables\\base.py:4629\u001B[39m, in \u001B[36mRunnableLambda._invoke\u001B[39m\u001B[34m(self, input_, run_manager, config, **kwargs)\u001B[39m\n\u001B[32m   4627\u001B[39m                 output = chunk\n\u001B[32m   4628\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m4629\u001B[39m     output = \u001B[43mcall_func_with_variable_args\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   4630\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\n\u001B[32m   4631\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   4632\u001B[39m \u001B[38;5;66;03m# If the output is a Runnable, invoke it\u001B[39;00m\n\u001B[32m   4633\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(output, Runnable):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_core\\runnables\\config.py:428\u001B[39m, in \u001B[36mcall_func_with_variable_args\u001B[39m\u001B[34m(func, input, config, run_manager, **kwargs)\u001B[39m\n\u001B[32m    426\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m run_manager \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m accepts_run_manager(func):\n\u001B[32m    427\u001B[39m     kwargs[\u001B[33m\"\u001B[39m\u001B[33mrun_manager\u001B[39m\u001B[33m\"\u001B[39m] = run_manager\n\u001B[32m--> \u001B[39m\u001B[32m428\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\oldAIProject\\.venv\\Lib\\site-packages\\llama_cpp\\llama.py:1902\u001B[39m, in \u001B[36m__call__\u001B[39m\u001B[34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001B[39m\n\u001B[32m   1840\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__call__\u001B[39m(\n\u001B[32m   1841\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   1842\u001B[39m     prompt: \u001B[38;5;28mstr\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1866\u001B[39m     logit_bias: Optional[Dict[\u001B[38;5;28mint\u001B[39m, \u001B[38;5;28mfloat\u001B[39m]] = \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m   1867\u001B[39m ) -> Union[CreateCompletionResponse, Iterator[CreateCompletionStreamResponse]]:\n\u001B[32m   1868\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Generate text from a prompt.\u001B[39;00m\n\u001B[32m   1869\u001B[39m \n\u001B[32m   1870\u001B[39m \u001B[33;03m    Args:\u001B[39;00m\n\u001B[32m   1871\u001B[39m \u001B[33;03m        prompt: The prompt to generate text from.\u001B[39;00m\n\u001B[32m   1872\u001B[39m \u001B[33;03m        suffix: A suffix to append to the generated text. If None, no suffix is appended.\u001B[39;00m\n\u001B[32m   1873\u001B[39m \u001B[33;03m        max_tokens: The maximum number of tokens to generate. If max_tokens <= 0 or None, the maximum number of tokens to generate is unlimited and depends on n_ctx.\u001B[39;00m\n\u001B[32m   1874\u001B[39m \u001B[33;03m        temperature: The temperature to use for sampling.\u001B[39;00m\n\u001B[32m   1875\u001B[39m \u001B[33;03m        top_p: The top-p value to use for nucleus sampling. Nucleus sampling described in academic paper \"The Curious Case of Neural Text Degeneration\" https://arxiv.org/abs/1904.09751\u001B[39;00m\n\u001B[32m   1876\u001B[39m \u001B[33;03m        min_p: The min-p value to use for minimum p sampling. Minimum P sampling as described in https://github.com/ggerganov/llama.cpp/pull/3841\u001B[39;00m\n\u001B[32m   1877\u001B[39m \u001B[33;03m        typical_p: The typical-p value to use for sampling. Locally Typical Sampling implementation described in the paper https://arxiv.org/abs/2202.00666.\u001B[39;00m\n\u001B[32m   1878\u001B[39m \u001B[33;03m        logprobs: The number of logprobs to return. If None, no logprobs are returned.\u001B[39;00m\n\u001B[32m   1879\u001B[39m \u001B[33;03m        echo: Whether to echo the prompt.\u001B[39;00m\n\u001B[32m   1880\u001B[39m \u001B[33;03m        stop: A list of strings to stop generation when encountered.\u001B[39;00m\n\u001B[32m   1881\u001B[39m \u001B[33;03m        frequency_penalty: The penalty to apply to tokens based on their frequency in the prompt.\u001B[39;00m\n\u001B[32m   1882\u001B[39m \u001B[33;03m        presence_penalty: The penalty to apply to tokens based on their presence in the prompt.\u001B[39;00m\n\u001B[32m   1883\u001B[39m \u001B[33;03m        repeat_penalty: The penalty to apply to repeated tokens.\u001B[39;00m\n\u001B[32m   1884\u001B[39m \u001B[33;03m        top_k: The top-k value to use for sampling. Top-K sampling described in academic paper \"The Curious Case of Neural Text Degeneration\" https://arxiv.org/abs/1904.09751\u001B[39;00m\n\u001B[32m   1885\u001B[39m \u001B[33;03m        stream: Whether to stream the results.\u001B[39;00m\n\u001B[32m   1886\u001B[39m \u001B[33;03m        seed: The seed to use for sampling.\u001B[39;00m\n\u001B[32m   1887\u001B[39m \u001B[33;03m        tfs_z: The tail-free sampling parameter. Tail Free Sampling described in https://www.trentonbricken.com/Tail-Free-Sampling/.\u001B[39;00m\n\u001B[32m   1888\u001B[39m \u001B[33;03m        mirostat_mode: The mirostat sampling mode.\u001B[39;00m\n\u001B[32m   1889\u001B[39m \u001B[33;03m        mirostat_tau: The target cross-entropy (or surprise) value you want to achieve for the generated text. A higher value corresponds to more surprising or less predictable text, while a lower value corresponds to less surprising or more predictable text.\u001B[39;00m\n\u001B[32m   1890\u001B[39m \u001B[33;03m        mirostat_eta: The learning rate used to update `mu` based on the error between the target and observed surprisal of the sampled word. A larger learning rate will cause `mu` to be updated more quickly, while a smaller learning rate will result in slower updates.\u001B[39;00m\n\u001B[32m   1891\u001B[39m \u001B[33;03m        model: The name to use for the model in the completion object.\u001B[39;00m\n\u001B[32m   1892\u001B[39m \u001B[33;03m        stopping_criteria: A list of stopping criteria to use.\u001B[39;00m\n\u001B[32m   1893\u001B[39m \u001B[33;03m        logits_processor: A list of logits processors to use.\u001B[39;00m\n\u001B[32m   1894\u001B[39m \u001B[33;03m        grammar: A grammar to use for constrained sampling.\u001B[39;00m\n\u001B[32m   1895\u001B[39m \u001B[33;03m        logit_bias: A logit bias to use.\u001B[39;00m\n\u001B[32m   1896\u001B[39m \n\u001B[32m   1897\u001B[39m \u001B[33;03m    Raises:\u001B[39;00m\n\u001B[32m   1898\u001B[39m \u001B[33;03m        ValueError: If the requested tokens exceed the context window.\u001B[39;00m\n\u001B[32m   1899\u001B[39m \u001B[33;03m        RuntimeError: If the prompt fails to tokenize or the model fails to evaluate the prompt.\u001B[39;00m\n\u001B[32m   1900\u001B[39m \n\u001B[32m   1901\u001B[39m \u001B[33;03m    Returns:\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1902\u001B[39m \u001B[33;03m        Response object containing the generated text.\u001B[39;00m\n\u001B[32m   1903\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m   1904\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.create_completion(\n\u001B[32m   1905\u001B[39m         prompt=prompt,\n\u001B[32m   1906\u001B[39m         suffix=suffix,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1929\u001B[39m         logit_bias=logit_bias,\n\u001B[32m   1930\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\oldAIProject\\.venv\\Lib\\site-packages\\llama_cpp\\llama.py:1835\u001B[39m, in \u001B[36mcreate_completion\u001B[39m\u001B[34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001B[39m\n\u001B[32m   1807\u001B[39m completion_or_chunks = \u001B[38;5;28mself\u001B[39m._create_completion(\n\u001B[32m   1808\u001B[39m     prompt=prompt,\n\u001B[32m   1809\u001B[39m     suffix=suffix,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1832\u001B[39m     logit_bias=logit_bias,\n\u001B[32m   1833\u001B[39m )\n\u001B[32m   1834\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m stream:\n\u001B[32m-> \u001B[39m\u001B[32m1835\u001B[39m     chunks: Iterator[CreateCompletionStreamResponse] = completion_or_chunks\n\u001B[32m   1836\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m chunks\n\u001B[32m   1837\u001B[39m completion: Completion = \u001B[38;5;28mnext\u001B[39m(completion_or_chunks)  \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\oldAIProject\\.venv\\Lib\\site-packages\\llama_cpp\\llama.py:1189\u001B[39m, in \u001B[36m_create_completion\u001B[39m\u001B[34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001B[39m\n\u001B[32m   1186\u001B[39m     suffix = \"â˜º\" + suffix\n\u001B[32m   1187\u001B[39m     suffix_space_prefix = 2\n\u001B[32m-> \u001B[39m\u001B[32m1189\u001B[39m # If prompt is empty, initialize completion with BOS token to avoid\n\u001B[32m   1190\u001B[39m # detokenization including a space at the beginning of the completion\n\u001B[32m   1191\u001B[39m completion_tokens: List[int] = [] if len(prompt) > 0 else [bos_token_id]\n\u001B[32m   1192\u001B[39m # Add blank space to start of prompt to match OG llama tokenizer\n",
      "\u001B[31mTypeError\u001B[39m: object of type 'StringPromptValue' has no len()"
     ]
    }
   ],
   "execution_count": 56
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
